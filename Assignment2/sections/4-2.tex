\section{Problem 4.2}
\paragraph{(a)}
Recognizing that the prior is conjugate to the binomial likelihood function, with observation $x$, we have the posterior distribution for $\theta$ as
\begin{align*}
    \pi(\theta \vert x) \sim \mathrm{Beta}(\alpha +x , \beta + n-x)
\end{align*}
Then the expected posterior loss is given by
\begin{align*}
    \mathbb{E}_{p(\theta \vert x)} L(\theta, d)
    &= \int L(\theta, d) \theta^{\alpha +x -1} (1-\theta)^{\beta +n -x +1} d\theta \cdot \frac{1}{\mathbf{B}(\alpha+x, \beta+n-x)}\\
    &=  k_1 \int_{0}^{d} (d-\theta)\theta^{\alpha +x -1} (1-\theta)^{\beta +n -x +1} d\theta \cdot \frac{1}{\mathbf{B}(\alpha+x, \beta+n-x)} \\
    &   \quad + k_2 \int_{d}^{1} (\theta-d)\theta^{\alpha +x -1} (1-\theta)^{\beta +n -x +1} d\theta \cdot \frac{1}{\mathbf{B}(\alpha+x, \beta+n-x)}\\
    &= k_1\left(d\mathrm{CDF}_{\alpha+x, \beta+n-x}(d) - \mathrm{CDF}_{\alpha+x+1, \beta+n-x}(d) \frac{\mathbf{B}(\alpha+x+1, \beta+n-x)}{\mathbf{B}(\alpha+x, \beta+n-x)} \right)\\
    &   \quad + k_2\left( (1-\mathrm{CDF}_{\alpha+x+1, \beta+n-x}(d))\frac{\mathbf{B}(\alpha+x+1, \beta+n-x)}{\mathbf{B}(\alpha+x, \beta+n-x)} - d(1-\mathrm{CDF}_{\alpha+x, \beta+n-x}(d))\right)\\
    &= k_1\left(d\mathrm{CDF}_{\alpha+x, \beta+n-x}(d) - \mathrm{CDF}_{\alpha+x+1, \beta+n-x}(d) \frac{\alpha+x}{\alpha+\beta+n} \right) \\
    &   \quad + k_2\left( (1-\mathrm{CDF}_{\alpha+x+1, \beta+n-x}(d))\frac{\alpha+x}{\alpha+\beta+n} - d(1-\mathrm{CDF}_{\alpha+x, \beta+n-x}(d))\right)
\end{align*}
where $\mathbf{B}(a, b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$, and $\mathrm{CDF}_{a, b}$ denotes the cummulative distribution function of distribution $\mathrm{Beta}(a, b)$.

The derivative of $\mathbb{E}_{p(\theta \vert x)} L(\theta, d)$ w.r.t $d$ is given by
\begin{align*}
    \frac{\partial \mathbb{E}_{p(x \vert \theta)} L(\theta, d)}{\partial d}
    &= k_1\left(\mathrm{CDF}_{\alpha+x, \beta+n-x}(d) + d\mathrm{PDF}_{\alpha+x, \beta+n-x}(d) - \frac{\alpha+x}{\alpha+\beta+n} \mathrm{PDF}_{\alpha+x+1, \beta+n-x}(d)\right)\\
    & \quad + k_2\left(- \frac{\alpha+x}{\alpha+\beta+n} \mathrm{PDF}_{\alpha+x+1, \beta+n-x}(d) - 1 + \mathrm{CDF}_{\alpha+x, \beta+n-x}(d) + d\mathrm{PDF}_{\alpha+x, \beta+n-x}(d) \right)\\
    &= -k_2 + (k_1+k_2) \mathrm{CDF}(d)
\end{align*}
where $\mathrm{PDF}_{a, b}$ denotes the probability density function of $\mathrm{Beta}(a, b)$.

The Bayes estimator is given when $\frac{\partial \mathbb{E}_{p(x \vert \theta)} L(\theta, d)}{\partial d} (\hat{\theta}^\pi) = 0$. Hence, we have
\begin{align*}
    \mathrm{CDF}_{\alpha+x, \beta+n-x}(\hat{\theta}^{\pi}) = \frac{k_2}{k_1+k_2}\\
\end{align*}
Hence, the Bayes estimator for $\theta$ is given by the $\frac{k_2}{k_1+k_2}$ quantile of the posterior distribution $\mathrm{Beta}(\alpha+x, \beta+n-x)$.

\paragraph{(b)}
Let $k_1 = k_2 = k$. The Bayes estimator satisfies
\begin{align*}
    \mathrm{CDF}_{\alpha+x, \beta+n-x}(\hat{\theta}^{\pi}) = \frac{1}{2}\\
\end{align*}
Recall that the median of $\mathrm{Beta}(a, b)$ is approximately given by $\frac{a-\frac{1}{3}}{a+b-\frac{2}{3}}$, the Bayes estimator is given by
\begin{align*}
    \hat{\theta}^{\pi}(x) = \frac{\alpha+x - \frac{1}{3}}{\alpha+\beta+n - \frac{2}{3}}
\end{align*}

\paragraph{(c)}
The use of asymmetric loss is advantageous when the repercussions of overestimation differ significantly from those of underestimation. By allocating larger coefficients to the components that could lead to more serious consequences, the frequency of such adverse outcomes may be diminished. An real-life example is medical diagnosis, in which the use of an asymmetric loss function is crucial because the costs of false negatives and false positives are highly imbalanced. 

\paragraph{(d)}
The risk for estimator $\hat{\theta}$ given $\theta_0$ is defined as
\begin{align*}
    R(\theta_0, \hat{\theta}) 
    &\coloneqq \mathbb{E}_{p(X \vert \theta_0)} L(\theta_0, \hat{\theta})\\
    &= \sum_{x=0}^{n} L(\theta_0, \hat{\theta}) \begin{pmatrix} n \\ x \end{pmatrix} \theta_0^{x} (1-\theta_0)^{n-x}
\end{align*}

{\color{red} tobecontinued}